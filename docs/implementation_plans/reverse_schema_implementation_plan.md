# SpecQL Reverse-Schema Implementation Plan

## Executive Summary

Design and implement `specql reverse-schema` command to convert PrintOptim's 566 SQL files organized in a hierarchical file structure (with embedded table codes) into SpecQL YAML entities with complete metadata preservation.

**Complexity**: Simple â†’ Medium (3-5 hours implementation)
**Key Challenge**: Hierarchical path parsing and table code extraction from COMMENT metadata

---

## Problem Analysis

### PrintOptim Migration Structure

```
../printoptim_backend_migration/db/0_schema/
â”œâ”€â”€ 01_write_side/           # Tables (CQRS write side)
â”‚   â”œâ”€â”€ 010_i18n/           # Domain: Internationalization
â”‚   â”‚   â”œâ”€â”€ 0101_locale/    # Sub-domain: Locale
â”‚   â”‚   â”‚   â”œâ”€â”€ 01011_language/          # Entity group
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ 010111_tb_language.sql  # File code: 010111
â”‚   â”‚   â”‚   â””â”€â”€ 01012_locale/
â”‚   â”‚   â”‚       â””â”€â”€ 010121_tb_locale.sql    # File code: 010121
â”‚   â”‚   â”œâ”€â”€ 0102_country/
â”‚   â”‚   â”‚   â”œâ”€â”€ 01021_continent/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 010211_tb_continent.sql  # File code: 010211
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ 010212_tl_continent.sql  # Translation table
â”‚   â”‚   â”‚   â”œâ”€â”€ 01022_country/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 010221_tb_country.sql
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ 010222_tl_country.sql
â”‚   â”‚   â”‚   â””â”€â”€ 01023_country_locale/
â”‚   â”‚   â”‚       â””â”€â”€ 010231_tb_country_locale.sql
â”œâ”€â”€ 02_query_side/           # Views (CQRS read side)
â”‚   â”œâ”€â”€ 020_base_views/
â”‚   â”œâ”€â”€ 021_common_dim/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 03_functions/            # Business logic functions
â”‚   â”œâ”€â”€ 035_scd/
â”‚   â”‚   â””â”€â”€ 03501_allocation/
â”‚   â”‚       â”œâ”€â”€ 035017_allocate_to_stock.sql
â”‚   â”‚       â””â”€â”€ 035018_get_or_create_stock_location.sql
â”œâ”€â”€ 04_turbo_router/
â””â”€â”€ 05_lazy_caching/
```

### Numbering System

**Hierarchical numbering pattern:**
- **Level 1 (3 digits)**: `010` = i18n domain
- **Level 2 (4 digits)**: `0101` = locale sub-domain
- **Level 3 (5 digits)**: `01011` = language entity group
- **Level 4 (6 digits)**: `010111` = tb_language table

**File naming convention:**
```
{table_code}_{table_type}_{entity_name}.sql

Examples:
010111_tb_language.sql      â†’ table_code: 010111, type: tb (base table)
010212_tl_continent.sql     â†’ table_code: 010212, type: tl (translation table)
01042_fn_format_address.sql â†’ table_code: 01042,  type: fn (function)
```

### SQL Structure with Metadata

```sql
CREATE TABLE catalog.tb_language (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    pk_language UUID DEFAULT gen_random_uuid() NOT NULL,
    identifier VARCHAR(255) NOT NULL,
    name VARCHAR(20),
    iso_code VARCHAR(10),

    -- Audit fields (Trinity pattern)
    created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    created_by UUID,
    updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    updated_by UUID,
    deleted_at TIMESTAMPTZ,
    deleted_by UUID,

    CONSTRAINT tb_language_identifier_key UNIQUE (identifier),
    CONSTRAINT tb_language_pk_language_key UNIQUE (pk_language)
);

COMMENT ON TABLE catalog.tb_language IS '[Table: 010111 | Write-Side.Common.Locale.Language] Defines supported languages...';
COMMENT ON COLUMN catalog.tb_language.pk_language IS 'Public UUID used for joins and external references.';
-- ... more column comments
```

**Comment metadata pattern:**
```
[Table: {table_code} | {category}.{domain_path}] {description}

Example:
[Table: 010111 | Write-Side.Common.Locale.Language] Defines supported languages...
         ^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
         code     domain hierarchy (dot-separated)
```

---

## Design Requirements

### 1. Input Processing
- Recursively scan directory tree for `*.sql` files
- Parse file path to extract hierarchical structure
- Extract table code from both:
  - Filename: `010111_tb_language.sql`
  - COMMENT metadata: `[Table: 010111 | ...]`
- Validate consistency between filename code and COMMENT code

### 2. Metadata Extraction

**From file path:**
```
01_write_side/010_i18n/0101_locale/01011_language/010111_tb_language.sql

Extract:
- Category: "write_side" (or "query_side", "functions")
- Domain path: ["010_i18n", "0101_locale", "01011_language"]
- Domain labels: ["i18n", "locale", "language"]
- Table code: "010111" (from filename)
- Table type: "tb" (base table) / "tl" (translation) / "fn" (function)
- Entity name: "language"
```

**From COMMENT:**
```sql
COMMENT ON TABLE catalog.tb_language IS '[Table: 010111 | Write-Side.Common.Locale.Language] Defines...';

Extract:
- Table code: "010111" (validate matches filename)
- Category: "Write-Side"
- Domain hierarchy: ["Common", "Locale", "Language"]
- Description: "Defines supported languages..."
```

**From CREATE TABLE:**
- Schema name: `catalog`
- Table name: `tb_language`
- Columns (excluding Trinity/audit fields)
- Constraints
- Foreign keys

### 3. Output YAML Structure

```yaml
entity: Language
schema: catalog
description: "Defines supported languages, following ISO 639-1/639-2 standards."

# NEW: Organization metadata
organization:
  table_code: "010111"           # From COMMENT/filename
  category: "write_side"         # From file path
  domain_path:                   # From file path
    - "i18n"
    - "locale"
    - "language"
  domain_hierarchy:              # From COMMENT (alternative representation)
    - "Write-Side"
    - "Common"
    - "Locale"
    - "Language"
  file_path: "01_write_side/010_i18n/0101_locale/01011_language/010111_tb_language.sql"
  table_type: "tb"               # tb, tl, fn

fields:
  name: text
  iso_code: text
  # ... (Trinity/audit fields auto-excluded)

# Actions would be extracted from 03_functions/ if applicable
actions: []

_metadata:
  source: "reverse-schema"
  generated_at: "2025-11-16T17:45:00Z"
  original_schema: "catalog"
  original_table: "tb_language"
```

---

## Implementation Architecture

### Phase 1: Core Components

#### 1.1 File Path Parser (`src/parsers/plpgsql/file_path_parser.py`)

**Purpose**: Extract hierarchical metadata from file paths

```python
@dataclass
class FilePath Metadata:
    """Metadata extracted from file path"""

    category: str                    # "write_side", "query_side", "functions"
    domain_path: List[str]           # ["010_i18n", "0101_locale", ...]
    domain_labels: List[str]         # ["i18n", "locale", "language"]
    table_code: str                  # "010111"
    table_type: str                  # "tb", "tl", "fn"
    entity_name: str                 # "language"
    full_path: str                   # Original file path
    relative_path: str               # Relative to root

class FilePathParser:
    """Parse PrintOptim hierarchical file paths"""

    def parse_path(self, file_path: Path, root_dir: Path) -> FilePathMetadata:
        """
        Parse file path to extract organizational metadata

        Example:
            Input: ../db/0_schema/01_write_side/010_i18n/0101_locale/01011_language/010111_tb_language.sql

            Returns:
                FilePathMetadata(
                    category="write_side",
                    domain_path=["010_i18n", "0101_locale", "01011_language"],
                    domain_labels=["i18n", "locale", "language"],
                    table_code="010111",
                    table_type="tb",
                    entity_name="language",
                    ...
                )
        """

        # 1. Get relative path from root
        rel_path = file_path.relative_to(root_dir)

        # 2. Extract category (01_write_side â†’ "write_side")
        category = self._extract_category(rel_path.parts[0])

        # 3. Extract domain path (all intermediate directories)
        domain_path = list(rel_path.parts[1:-1])

        # 4. Extract domain labels (strip numbering)
        domain_labels = [self._strip_numbering(d) for d in domain_path]

        # 5. Parse filename
        filename = file_path.stem  # "010111_tb_language"
        table_code, table_type, entity_name = self._parse_filename(filename)

        return FilePathMetadata(...)

    def _extract_category(self, dir_name: str) -> str:
        """01_write_side â†’ write_side"""
        return re.sub(r'^\d+_', '', dir_name)

    def _strip_numbering(self, dir_name: str) -> str:
        """010_i18n â†’ i18n, 01011_language â†’ language"""
        return re.sub(r'^\d+_', '', dir_name)

    def _parse_filename(self, filename: str) -> Tuple[str, str, str]:
        """
        010111_tb_language â†’ ("010111", "tb", "language")
        01042_fn_format_address â†’ ("01042", "fn", "format_address")
        """
        pattern = r'^(\d+)_(tb|tl|fn|vw)_(.+)$'
        match = re.match(pattern, filename)

        if not match:
            raise ValueError(f"Invalid filename format: {filename}")

        return match.group(1), match.group(2), match.group(3)
```

#### 1.2 COMMENT Metadata Extractor (`src/parsers/plpgsql/comment_parser.py`)

**Purpose**: Extract table code and domain hierarchy from COMMENT statements

```python
@dataclass
class CommentMetadata:
    """Metadata extracted from COMMENT ON TABLE"""

    table_code: str                  # "010111"
    category: str                    # "Write-Side"
    domain_hierarchy: List[str]      # ["Common", "Locale", "Language"]
    description: str                 # Main description text

class CommentParser:
    """Parse PostgreSQL COMMENT metadata"""

    COMMENT_PATTERN = r"COMMENT\s+ON\s+TABLE\s+\S+\s+IS\s+'(.+?)'"
    METADATA_PATTERN = r'\[Table:\s*(\d+)\s*\|\s*([^\]]+)\]\s*(.+)'

    def extract_comment_metadata(self, sql: str) -> Optional[CommentMetadata]:
        """
        Extract metadata from COMMENT ON TABLE statement

        Example:
            Input: COMMENT ON TABLE catalog.tb_language IS
                   '[Table: 010111 | Write-Side.Common.Locale.Language] Defines...';

            Returns:
                CommentMetadata(
                    table_code="010111",
                    category="Write-Side",
                    domain_hierarchy=["Common", "Locale", "Language"],
                    description="Defines supported languages..."
                )
        """

        # 1. Extract full COMMENT text
        comment_match = re.search(self.COMMENT_PATTERN, sql, re.IGNORECASE | re.DOTALL)
        if not comment_match:
            return None

        comment_text = comment_match.group(1)

        # 2. Parse metadata section
        metadata_match = re.match(self.METADATA_PATTERN, comment_text)
        if not metadata_match:
            # No metadata, just description
            return CommentMetadata(
                table_code="",
                category="",
                domain_hierarchy=[],
                description=comment_text.strip()
            )

        table_code = metadata_match.group(1)
        hierarchy_str = metadata_match.group(2)
        description = metadata_match.group(3).strip()

        # 3. Parse domain hierarchy (Write-Side.Common.Locale.Language)
        parts = hierarchy_str.split('.')
        category = parts[0] if parts else ""
        domain_hierarchy = parts[1:] if len(parts) > 1 else []

        return CommentMetadata(
            table_code=table_code,
            category=category,
            domain_hierarchy=domain_hierarchy,
            description=description
        )
```

#### 1.3 Enhanced Schema Analyzer (`src/parsers/plpgsql/schema_analyzer.py`)

**Update existing class** to include organization metadata:

```python
class SchemaAnalyzer:
    """Analyze PostgreSQL DDL schemas"""

    def __init__(self):
        self.file_path_parser = FilePathParser()
        self.comment_parser = CommentParser()

    def parse_create_table_with_metadata(
        self,
        ddl: str,
        file_path: Optional[Path] = None,
        root_dir: Optional[Path] = None
    ) -> UniversalEntity:
        """
        Parse CREATE TABLE with full organizational metadata

        Args:
            ddl: CREATE TABLE statement
            file_path: Path to SQL file (optional)
            root_dir: Root directory for relative path calculation (optional)

        Returns:
            UniversalEntity with organization metadata
        """

        # 1. Parse basic table structure (existing logic)
        entity = self.parse_create_table(ddl)

        # 2. Extract COMMENT metadata
        comment_meta = self.comment_parser.extract_comment_metadata(ddl)

        # 3. Extract file path metadata
        path_meta = None
        if file_path and root_dir:
            path_meta = self.file_path_parser.parse_path(file_path, root_dir)

        # 4. Merge metadata into entity
        entity = self._enrich_entity_with_metadata(entity, comment_meta, path_meta)

        return entity

    def _enrich_entity_with_metadata(
        self,
        entity: UniversalEntity,
        comment_meta: Optional[CommentMetadata],
        path_meta: Optional[FilePathMetadata]
    ) -> UniversalEntity:
        """Merge metadata from multiple sources"""

        # Create organization dict
        organization = {}

        # From file path
        if path_meta:
            organization['table_code'] = path_meta.table_code
            organization['category'] = path_meta.category
            organization['domain_path'] = path_meta.domain_labels
            organization['file_path'] = str(path_meta.relative_path)
            organization['table_type'] = path_meta.table_type

        # From COMMENT (override if present)
        if comment_meta:
            if comment_meta.table_code:
                organization['table_code'] = comment_meta.table_code
            if comment_meta.category:
                organization['category'] = comment_meta.category
            if comment_meta.domain_hierarchy:
                organization['domain_hierarchy'] = comment_meta.domain_hierarchy
            if comment_meta.description:
                entity.description = comment_meta.description

        # Validate consistency
        if path_meta and comment_meta:
            if comment_meta.table_code and path_meta.table_code != comment_meta.table_code:
                warnings.warn(
                    f"Table code mismatch: file={path_meta.table_code}, "
                    f"comment={comment_meta.table_code}"
                )

        # Attach to entity (need to extend UniversalEntity)
        entity.organization = organization

        return entity
```

#### 1.4 Update Universal AST (`src/core/universal_ast.py`)

**Add organization field to UniversalEntity:**

```python
@dataclass
class UniversalEntity:
    """Framework-agnostic entity definition"""

    name: str
    schema: str
    fields: List[UniversalField]
    actions: List["UniversalAction"]

    # Multi-tenancy
    is_multi_tenant: bool = True

    # Metadata
    description: Optional[str] = None

    # NEW: Organizational metadata (for reverse engineering)
    organization: Optional[Dict[str, Any]] = None
    # Structure:
    # {
    #   'table_code': '010111',
    #   'category': 'write_side',
    #   'domain_path': ['i18n', 'locale', 'language'],
    #   'domain_hierarchy': ['Write-Side', 'Common', 'Locale', 'Language'],
    #   'file_path': '01_write_side/010_i18n/.../010111_tb_language.sql',
    #   'table_type': 'tb'
    # }
```

### Phase 2: CLI Command

#### 2.1 CLI Interface (`src/cli/commands/reverse_schema.py`)

```python
"""
CLI command for reverse engineering SQL schemas to SpecQL YAML

Usage:
    specql reverse-schema path/to/schema/ --output-dir entities/
    specql reverse-schema db/0_schema/01_write_side --output-dir entities/ --recursive
"""

import click
from pathlib import Path
from typing import List
from src.parsers.plpgsql.schema_analyzer import SchemaAnalyzer
from src.core.specql_generator import SpecQLGenerator

@click.command()
@click.argument('input_path', type=click.Path(exists=True))
@click.option(
    '--output-dir', '-o',
    type=click.Path(),
    required=True,
    help='Output directory for YAML files'
)
@click.option(
    '--recursive/--no-recursive',
    default=True,
    help='Recursively scan subdirectories'
)
@click.option(
    '--extract-table-codes/--no-extract-table-codes',
    default=True,
    help='Extract table codes from filenames and COMMENTs'
)
@click.option(
    '--preserve-hierarchy/--flatten',
    default=False,
    help='Preserve directory hierarchy in output (default: flatten to single dir)'
)
@click.option(
    '--filter-category',
    type=click.Choice(['write_side', 'query_side', 'functions', 'all']),
    default='all',
    help='Filter by category (default: all)'
)
@click.option(
    '--exclude-translations/--include-translations',
    default=True,
    help='Exclude translation tables (tl_*)'
)
@click.option(
    '--preview',
    is_flag=True,
    help='Preview mode (no files written)'
)
@click.option(
    '--validate-consistency/--no-validate-consistency',
    default=True,
    help='Validate table code consistency between filename and COMMENT'
)
def reverse_schema(
    input_path: str,
    output_dir: str,
    recursive: bool,
    extract_table_codes: bool,
    preserve_hierarchy: bool,
    filter_category: str,
    exclude_translations: bool,
    preview: bool,
    validate_consistency: bool
):
    """
    Reverse engineer PostgreSQL schema to SpecQL YAML entities

    Examples:
        # Convert entire PrintOptim schema
        specql reverse-schema ../printoptim_backend_migration/db/0_schema \\
            --output-dir entities/ --recursive

        # Only write-side tables, exclude translations
        specql reverse-schema db/0_schema/01_write_side \\
            --output-dir entities/ \\
            --filter-category write_side \\
            --exclude-translations

        # Preview mode (dry run)
        specql reverse-schema db/0_schema --output-dir entities/ --preview
    """

    input_path_obj = Path(input_path)
    output_path = Path(output_dir)

    click.echo(f"ðŸ”„ Scanning {input_path}...")

    # 1. Discover SQL files
    sql_files = _discover_sql_files(
        input_path_obj,
        recursive=recursive,
        filter_category=filter_category,
        exclude_translations=exclude_translations
    )

    click.echo(f"ðŸ“ Found {len(sql_files)} SQL files")

    # 2. Process each file
    analyzer = SchemaAnalyzer()
    entities = []
    errors = []

    for sql_file in sql_files:
        try:
            click.echo(f"  Processing {sql_file.name}...")

            # Read SQL
            sql = sql_file.read_text()

            # Parse with metadata
            entity = analyzer.parse_create_table_with_metadata(
                ddl=sql,
                file_path=sql_file,
                root_dir=input_path_obj
            )

            # Validate consistency
            if validate_consistency and entity.organization:
                _validate_table_code_consistency(entity, sql_file)

            entities.append((entity, sql_file))

        except Exception as e:
            errors.append((sql_file, str(e)))
            click.echo(f"    âŒ Error: {e}")

    # 3. Generate YAML files
    if not preview:
        _write_yaml_files(entities, output_path, preserve_hierarchy, input_path_obj)
    else:
        _preview_yaml_files(entities)

    # 4. Summary
    _print_summary(entities, errors)


def _discover_sql_files(
    root: Path,
    recursive: bool,
    filter_category: str,
    exclude_translations: bool
) -> List[Path]:
    """Discover SQL files matching criteria"""

    pattern = "**/*.sql" if recursive else "*.sql"
    all_files = list(root.glob(pattern))

    # Filter by category
    if filter_category != 'all':
        category_prefix = f"{_get_category_prefix(filter_category)}_"
        all_files = [
            f for f in all_files
            if any(category_prefix in str(part) for part in f.parts)
        ]

    # Exclude translations
    if exclude_translations:
        all_files = [
            f for f in all_files
            if not f.stem.split('_')[1] == 'tl'  # {code}_tl_{name}
        ]

    # Only table files (tb_*, tl_*, not fn_*)
    all_files = [
        f for f in all_files
        if f.stem.split('_')[1] in ['tb', 'tl']
    ]

    return sorted(all_files)


def _get_category_prefix(category: str) -> str:
    """Map category to directory prefix"""
    mapping = {
        'write_side': '01',
        'query_side': '02',
        'functions': '03'
    }
    return mapping.get(category, '')


def _validate_table_code_consistency(entity: UniversalEntity, file_path: Path):
    """Validate table code consistency between filename and COMMENT"""

    org = entity.organization or {}

    # Extract from filename
    filename_code = file_path.stem.split('_')[0]

    # Extract from organization
    comment_code = org.get('table_code', '')

    if comment_code and filename_code != comment_code:
        click.secho(
            f"    âš ï¸  Warning: Table code mismatch - "
            f"filename={filename_code}, comment={comment_code}",
            fg='yellow'
        )


def _write_yaml_files(
    entities: List[Tuple[UniversalEntity, Path]],
    output_dir: Path,
    preserve_hierarchy: bool,
    root_dir: Path
):
    """Write entities to YAML files"""

    output_dir.mkdir(parents=True, exist_ok=True)

    for entity, source_file in entities:
        # Determine output path
        if preserve_hierarchy:
            # Recreate directory structure
            rel_path = source_file.relative_to(root_dir)
            output_path = output_dir / rel_path.parent / f"{entity.name}.yaml"
            output_path.parent.mkdir(parents=True, exist_ok=True)
        else:
            # Flatten to output directory
            output_path = output_dir / f"{entity.name}.yaml"

        # Generate YAML
        yaml_content = _entity_to_yaml(entity)

        # Write file
        output_path.write_text(yaml_content)

        click.echo(f"    ðŸ’¾ Written to {output_path}")


def _entity_to_yaml(entity: UniversalEntity) -> str:
    """Convert UniversalEntity to SpecQL YAML"""

    from src.core.specql_generator import SpecQLGenerator

    generator = SpecQLGenerator()
    return generator.generate_yaml(entity)


def _preview_yaml_files(entities: List[Tuple[UniversalEntity, Path]]):
    """Preview YAML output without writing"""

    for entity, source_file in entities:
        click.echo(f"\n--- {entity.name}.yaml ---")
        yaml_content = _entity_to_yaml(entity)
        click.echo(yaml_content[:500])  # First 500 chars
        click.echo("...")


def _print_summary(
    entities: List[Tuple[UniversalEntity, Path]],
    errors: List[Tuple[Path, str]]
):
    """Print processing summary"""

    click.echo("\nðŸ“Š Summary:")
    click.echo(f"  âœ… Successfully processed: {len(entities)}")
    click.echo(f"  âŒ Errors: {len(errors)}")

    if errors:
        click.echo("\nâŒ Failed files:")
        for file_path, error in errors:
            click.echo(f"  â€¢ {file_path.name}: {error}")
```

#### 2.2 YAML Generator Updates (`src/core/specql_generator.py`)

**Add organization section to YAML output:**

```python
class SpecQLGenerator:
    """Generate SpecQL YAML from UniversalEntity"""

    def generate_yaml(self, entity: UniversalEntity) -> str:
        """Generate YAML with organization metadata"""

        yaml_dict = {
            'entity': entity.name,
            'schema': entity.schema,
        }

        # Add description if present
        if entity.description:
            yaml_dict['description'] = entity.description

        # Add organization metadata (NEW)
        if entity.organization:
            yaml_dict['organization'] = entity.organization

        # Add fields
        yaml_dict['fields'] = self._generate_fields(entity.fields)

        # Add actions
        if entity.actions:
            yaml_dict['actions'] = self._generate_actions(entity.actions)

        return yaml.dump(
            yaml_dict,
            default_flow_style=False,
            sort_keys=False,
            allow_unicode=True
        )
```

### Phase 3: Testing

#### 3.1 Unit Tests

```python
# tests/unit/parsers/plpgsql/test_file_path_parser.py

def test_parse_write_side_table():
    """Test parsing write-side table path"""

    parser = FilePathParser()

    file_path = Path("../printoptim/db/0_schema/01_write_side/010_i18n/0101_locale/01011_language/010111_tb_language.sql")
    root_dir = Path("../printoptim/db/0_schema")

    meta = parser.parse_path(file_path, root_dir)

    assert meta.category == "write_side"
    assert meta.domain_path == ["010_i18n", "0101_locale", "01011_language"]
    assert meta.domain_labels == ["i18n", "locale", "language"]
    assert meta.table_code == "010111"
    assert meta.table_type == "tb"
    assert meta.entity_name == "language"


# tests/unit/parsers/plpgsql/test_comment_parser.py

def test_extract_comment_metadata():
    """Test extracting metadata from COMMENT statement"""

    sql = """
    COMMENT ON TABLE catalog.tb_language IS '[Table: 010111 | Write-Side.Common.Locale.Language] Defines supported languages...';
    """

    parser = CommentParser()
    meta = parser.extract_comment_metadata(sql)

    assert meta.table_code == "010111"
    assert meta.category == "Write-Side"
    assert meta.domain_hierarchy == ["Common", "Locale", "Language"]
    assert "Defines supported languages" in meta.description


# tests/integration/test_reverse_schema.py

def test_reverse_schema_full_pipeline():
    """Test complete reverse-schema pipeline"""

    # Create test SQL file
    sql_content = """
    CREATE TABLE catalog.tb_language (
        id INTEGER PRIMARY KEY,
        pk_language UUID NOT NULL,
        name VARCHAR(20)
    );

    COMMENT ON TABLE catalog.tb_language IS '[Table: 010111 | Write-Side.Common.Locale.Language] Defines languages.';
    """

    # ... write to temp file

    # Run reverse-schema
    result = run_cli([
        'reverse-schema',
        str(temp_dir),
        '--output-dir', str(output_dir),
        '--extract-table-codes'
    ])

    # Verify YAML output
    yaml_file = output_dir / "Language.yaml"
    assert yaml_file.exists()

    with open(yaml_file) as f:
        data = yaml.safe_load(f)

    assert data['entity'] == 'Language'
    assert data['organization']['table_code'] == '010111'
    assert data['organization']['category'] == 'write_side'
```

---

## Implementation Phases

### Phase 1: Core Parsers (2 hours)
- âœ… Create `FilePathParser` class
- âœ… Create `CommentParser` class
- âœ… Update `SchemaAnalyzer` to merge metadata
- âœ… Update `UniversalEntity` with `organization` field
- âœ… Write unit tests for parsers

### Phase 2: CLI Command (1.5 hours)
- âœ… Create `reverse_schema.py` CLI command
- âœ… Implement file discovery logic
- âœ… Implement batch processing
- âœ… Add filtering options (category, translations)
- âœ… Add validation logic

### Phase 3: YAML Generation (0.5 hours)
- âœ… Update `SpecQLGenerator` to include `organization` section
- âœ… Test YAML output format

### Phase 4: Integration Testing (1 hour)
- âœ… Test on PrintOptim sample files
- âœ… Validate table code consistency
- âœ… Test hierarchy preservation
- âœ… Test filtering options

**Total Estimated Time**: 5 hours

---

## Success Criteria

1. âœ… Successfully parse 566 PrintOptim SQL files
2. âœ… Extract table codes from both filenames and COMMENT metadata
3. âœ… Validate consistency between filename and COMMENT codes
4. âœ… Generate YAML with complete `organization` metadata
5. âœ… Support filtering by category (write_side, query_side, functions)
6. âœ… Exclude translation tables (tl_*) when requested
7. âœ… Preserve or flatten directory hierarchy based on option
8. âœ… Provide clear error messages for malformed files

---

## Example Usage

```bash
# Convert entire PrintOptim schema
specql reverse-schema ../printoptim_backend_migration/db/0_schema \
  --output-dir entities/printoptim \
  --recursive \
  --extract-table-codes

# Only write-side tables, exclude translations, preserve hierarchy
specql reverse-schema ../printoptim_backend_migration/db/0_schema/01_write_side \
  --output-dir entities/write_side \
  --filter-category write_side \
  --exclude-translations \
  --preserve-hierarchy

# Preview mode (dry run)
specql reverse-schema ../printoptim_backend_migration/db/0_schema \
  --output-dir entities/ \
  --preview
```

**Output YAML Example:**

```yaml
entity: Language
schema: catalog
description: "Defines supported languages, following ISO 639-1/639-2 standards. Used for translation, formatting, ..."

organization:
  table_code: "010111"
  category: "write_side"
  domain_path:
    - "i18n"
    - "locale"
    - "language"
  domain_hierarchy:
    - "Write-Side"
    - "Common"
    - "Locale"
    - "Language"
  file_path: "01_write_side/010_i18n/0101_locale/01011_language/010111_tb_language.sql"
  table_type: "tb"

fields:
  name: text
  iso_code: text

actions: []

_metadata:
  source: "reverse-schema"
  generated_at: "2025-11-16T17:45:00Z"
  original_schema: "catalog"
  original_table: "tb_language"
```

---

## Future Enhancements

1. **Function extraction**: Parse `03_functions/` directory to extract business logic
2. **View mapping**: Parse `02_query_side/` views and link to write-side entities
3. **Relationship detection**: Auto-detect foreign keys and generate `ref()` fields
4. **Pattern detection**: Identify common patterns (i18n, SCD, etc.) from structure
5. **Forward generation**: Generate back to SQL from YAML to validate round-trip

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Table code mismatch between filename and COMMENT | Add validation flag `--validate-consistency` with warnings |
| Malformed COMMENT metadata | Gracefully fallback to filename-only parsing |
| Missing directory hierarchy | Make all path-based metadata optional |
| Large file count (566 files) | Add progress bar, batch processing, error recovery |
| Inconsistent file naming | Support multiple patterns, log warnings for unrecognized formats |

---

**Ready to implement?** This plan provides complete implementation details for converting PrintOptim's hierarchical SQL schema to SpecQL YAML with full metadata preservation.
